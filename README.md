# Samsung EnnovateX 2025 AI Challenge Submission

- **Problem Statement:** On-Device Fine-Tuning Framework for Billion+-Parameter LLMs
- **Team Name:** PocketLoRA Innovators
- **Team Members:** Karthick Kumarasamy
- **Demo Video Link:** TBD

## Project Artefacts

- **Technical Documentation:** `/docs/technical_design.md`, `/docs/evaluation_plan.md`
- **Source Code:** `/src/AndroidApp`, `/src/AdapterTrainer`
- **Models Used:** Llama 3.2 3B (Hugging Face link), Gemma-2 2B
- **Models Published:** [Link to HuggingFace if you publish adapters]
- **Datasets Used:** Private user message dataset (locally generated)
- **Datasets Published:** None (privacy-preserving)
- **Attribution:** ExecuTorch, ONNX Runtime Training, QLoRA/LoRA techniques


# PocketLoRA: On-Device Fine-Tuning for Billion-Scale LLMs

ğŸ“± **PocketLoRA** is an efficient framework for **on-device fine-tuning** of Billion+ parameter scale LLMs on **Galaxy S23â€“S25 class smartphones** (or equivalent edge devices).  
It enables adapting pre-trained LLMs to a userâ€™s **personal data** while preserving **privacy, efficiency, and usability**.

---

## ğŸ“Œ Problem Statement
Large Language Models (LLMs) are powerful, but:
- They are trained on **general-purpose data**, not personal context.
- Fine-tuning typically requires **cloud GPUs**, raising cost and privacy issues.
- Smartphones have **tight memory, compute, and thermal constraints**.

**Goal:** Enable practical **adapter-based fine-tuning** of LLMs fully **on-device**, respecting the constraints of mobile hardware.

---

## ğŸš€ Our Approach
- Use **quantized base models (NF4)** with memory mapping to reduce RAM usage.
- Train **LoRA / QLoRA adapters** (20â€“150 MB) instead of full models.
- Implement **thermal-aware scheduling**: train only when charging, Wi-Fi, and screen-off.
- Maintain **privacy guarantees**: no raw text leaves device, only adapters exported.
- Provide a **user-friendly app UI** to manage adapters.

ğŸ‘‰ Details: [docs/approach.md](docs/approach.md)

---

## ğŸ› ï¸ Technical Stack
- **Frameworks**: PyTorch Mobile, ExecuTorch, ONNX Runtime Mobile  
- **Quantization**: bitsandbytes (NF4), GPTQ  
- **Fine-Tuning**: LoRA / QLoRA  
- **Optimizers**: Adafactor, Lion  
- **UI**: Android (Kotlin + Jetpack Compose)  

ğŸ‘‰ Full list: [docs/technical_stack.md](docs/technical_stack.md)

---

## ğŸ“ Architecture
The architecture consists of:
1. **Data Ingestion** â†’ 2. **Local Dataset Store** â†’  
3. **Adapter Trainer** â†’ 4. **Adapter Store** â†’  
5. **Inference Runtime** â†’ 6. **Scheduler & Privacy Controls**

![System Flow](docs/screenshots/system_flow.png)

ğŸ‘‰ More details: [docs/architecture.md](docs/architecture.md)

---

## âš™ï¸ Implementation
- Pseudocode for loading, training, saving, and inference.
- Optimizations: quantization, micro-batching, checkpointing.
- Thermal/power-aware scheduler.  

![Training Loop](docs/screenshots/training_loop.png)

ğŸ‘‰ Full pseudocode: [docs/implementation.md](docs/implementation.md)

---

## ğŸ“² Installation
Step-by-step setup:
1. Clone repo.  
2. Quantize base model to NF4.  
3. Build and install Android app.  

ğŸ‘‰ Setup guide: [docs/installation.md](docs/installation.md)

---

## ğŸ§‘â€ğŸ’» User Guide
- **Start training** with personal text.  
- **Monitor progress** via training state (Idle, Training, Cooldown).  
- **Hot-swap adapters** during inference.  
- **Manage adapters**: enable, export, delete.  

![App UI](docs/screenshots/app_ui.png)

ğŸ‘‰ Walkthrough: [docs/user_guide.md](docs/user_guide.md)

---

## âœ¨ Key Features
- ğŸ”’ Fully **on-device** (privacy-preserving).  
- âš¡ **Resource-efficient** (â‰¤ 6 GB RAM peak).  
- ğŸ›¡ï¸ **Thermal & power-aware** scheduling.  
- ğŸ›ï¸ Multiple adapter management.  
- ğŸ“± Clean **mobile UI** with monitoring.  

ğŸ‘‰ See: [docs/features.md](docs/features.md)

---

## ğŸ“‚ Repository Structure


## ğŸ“‚ Repository Structure

.
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ approach.md
â”‚ â”œâ”€â”€ technical_stack.md
â”‚ â”œâ”€â”€ architecture.md
â”‚ â”œâ”€â”€ implementation.md
â”‚ â”œâ”€â”€ installation.md
â”‚ â”œâ”€â”€ user_guide.md
â”‚ â”œâ”€â”€ features.md
â”‚ â””â”€â”€ screenshots/
â”‚ â”œâ”€â”€ system_flow.png
â”‚ â”œâ”€â”€ memory_budget.png
â”‚ â”œâ”€â”€ app_ui.png
â”‚ â”œâ”€â”€ training_loop.png
â”‚ â””â”€â”€ adapter_lifecycle.png
â”œâ”€â”€ app/ # Android app source
â”œâ”€â”€ tools/ # Model quantization & preprocessing scripts
â”œâ”€â”€ models/ # Base model (NF4 quantized, not checked in)
â””â”€â”€ adapters/ # Trained LoRA adapters



---

## ğŸ“œ License
Released under the [MIT License](LICENSE).

---

## ğŸ† Submission Notes
This repository is part of the **Samsung EnnovateX 2025 AI Challenge** under Problem Statement #5:  
_On-Device Fine-Tuning Framework for Billion+ Parameter LLMs._
